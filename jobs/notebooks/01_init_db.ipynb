{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409d3d-41a6-47ec-9474-b392da225d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1022aabc-ae7a-4d69-86de-4fc9f9bc3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "print(sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf1d053-f736-4c10-a0bc-f3e579a5568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:derby:;databaseName=/opt/spark/spark-warehouse/metastore_db;create=true\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.hadoopConfiguration().get(\"javax.jdo.option.ConnectionURL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843b423d-3ba9-48fa-b11b-b9f74a986282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/spark-warehouse/\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.hadoopConfiguration().get(\"hive.metastore.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d9b200-04b2-4bf9-89e3-e648f52c210b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 02:34:18 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('/opt/spark/spark-warehouse/') to the value of spark.sql.warehouse.dir.\n",
      "25/05/23 02:34:18 WARN  MetricsConfig:136 Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/23 02:34:18 INFO  MetricsSystemImpl:378 Scheduled Metric snapshot period at 10 second(s).\n",
      "25/05/23 02:34:18 INFO  MetricsSystemImpl:191 s3a-file-system metrics system started\n",
      "25/05/23 02:34:19 WARN  VersionInfoUtils:85 The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1258)\n",
      "at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:898)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)\n",
      "at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
      "at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "at org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n",
      "at org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/23 02:34:23 INFO  SharedState:60 Warehouse path is 's3a://delta-datawarehouse'.\n",
      "25/05/23 02:34:24 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:38512) with ID 2,  ResourceProfileId 0\n",
      "25/05/23 02:34:24 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:39646) with ID 1,  ResourceProfileId 0\n",
      "25/05/23 02:34:24 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:48990) with ID 0,  ResourceProfileId 0\n",
      "25/05/23 02:34:24 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.5:36501 with 1048.8 MiB RAM, BlockManagerId(2, 172.18.0.5, 36501, None)\n",
      "25/05/23 02:34:24 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.7:36733 with 1048.8 MiB RAM, BlockManagerId(1, 172.18.0.7, 36733, None)\n",
      "25/05/23 02:34:24 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.4:35581 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.4, 35581, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hive\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.catalogImplementation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f21b504-be39-40df-9a4b-5bddb8355658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 02:34:25 INFO  SparkContext:60 SparkContext is stopping with exitCode 0.\n",
      "25/05/23 02:34:25 INFO  SparkUI:60 Stopped Spark web UI at http://db74469cb85a:4040\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend:60 Shutting down all executors\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Asking each executor to shut down\n",
      "25/05/23 02:34:25 INFO  MapOutputTrackerMasterEndpoint:60 MapOutputTrackerMasterEndpoint stopped!\n",
      "25/05/23 02:34:25 INFO  MemoryStore:60 MemoryStore cleared\n",
      "25/05/23 02:34:25 INFO  BlockManager:60 BlockManager stopped\n",
      "25/05/23 02:34:25 INFO  BlockManagerMaster:60 BlockManagerMaster stopped\n",
      "25/05/23 02:34:25 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 OutputCommitCoordinator stopped!\n",
      "25/05/23 02:34:25 INFO  SparkContext:60 Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6825187ec98f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 02:34:25 INFO  SparkContext:60 Running Spark version 3.5.5\n",
      "25/05/23 02:34:25 INFO  SparkContext:60 OS info Linux, 5.15.146.1-microsoft-standard-WSL2, amd64\n",
      "25/05/23 02:34:25 INFO  SparkContext:60 Java version 17.0.14\n",
      "Setting Spark log level to \"INFO\".\n",
      "25/05/23 02:34:25 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/23 02:34:25 INFO  ResourceUtils:60 No custom resources configured for spark.driver.\n",
      "25/05/23 02:34:25 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/23 02:34:25 INFO  SparkContext:60 Submitted application: Init DB\n",
      "25/05/23 02:34:25 INFO  ResourceProfile:60 Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/05/23 02:34:25 INFO  ResourceProfile:60 Limiting resource is cpus at 4 tasks per executor\n",
      "25/05/23 02:34:25 INFO  ResourceProfileManager:60 Added ResourceProfile id: 0\n",
      "25/05/23 02:34:25 INFO  SecurityManager:60 Changing view acls to: root\n",
      "25/05/23 02:34:25 INFO  SecurityManager:60 Changing modify acls to: root\n",
      "25/05/23 02:34:25 INFO  SecurityManager:60 Changing view acls groups to: \n",
      "25/05/23 02:34:25 INFO  SecurityManager:60 Changing modify acls groups to: \n",
      "25/05/23 02:34:25 INFO  SecurityManager:60 SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/05/23 02:34:25 INFO  Utils:60 Successfully started service 'sparkDriver' on port 34413.\n",
      "25/05/23 02:34:25 INFO  SparkEnv:60 Registering MapOutputTracker\n",
      "25/05/23 02:34:25 INFO  SparkEnv:60 Registering BlockManagerMaster\n",
      "25/05/23 02:34:25 INFO  BlockManagerMasterEndpoint:60 Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/05/23 02:34:25 INFO  BlockManagerMasterEndpoint:60 BlockManagerMasterEndpoint up\n",
      "25/05/23 02:34:25 INFO  SparkEnv:60 Registering BlockManagerMasterHeartbeat\n",
      "25/05/23 02:34:25 INFO  DiskBlockManager:60 Created local directory at /tmp/blockmgr-9daed99b-3f06-4e69-a694-e20705a9312b\n",
      "25/05/23 02:34:25 INFO  MemoryStore:60 MemoryStore started with capacity 434.4 MiB\n",
      "25/05/23 02:34:25 INFO  SparkEnv:60 Registering OutputCommitCoordinator\n",
      "25/05/23 02:34:25 INFO  JettyUtils:60 Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/05/23 02:34:25 INFO  Utils:60 Successfully started service 'SparkUI' on port 4040.\n",
      "25/05/23 02:34:25 INFO  StandaloneAppClient$ClientEndpoint:60 Connecting to master spark://delta-warehouse-spark-master:7077...\n",
      "25/05/23 02:34:25 INFO  TransportClientFactory:316 Successfully created connection to delta-warehouse-spark-master/172.18.0.3:7077 after 2 ms (0 ms spent in bootstraps)\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend:60 Connected to Spark cluster with app ID app-20250523023425-0005\n",
      "25/05/23 02:34:25 INFO  Utils:60 Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42557.\n",
      "25/05/23 02:34:25 INFO  NettyBlockTransferService:84 Server created on db74469cb85a:42557\n",
      "25/05/23 02:34:25 INFO  BlockManager:60 Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/05/23 02:34:25 INFO  BlockManagerMaster:60 Registering BlockManager BlockManagerId(driver, db74469cb85a, 42557, None)\n",
      "25/05/23 02:34:25 INFO  BlockManagerMasterEndpoint:60 Registering block manager db74469cb85a:42557 with 434.4 MiB RAM, BlockManagerId(driver, db74469cb85a, 42557, None)\n",
      "25/05/23 02:34:25 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250523023425-0005/0 on worker-20250523021808-172.18.0.4-38149 (172.18.0.4:38149) with 4 core(s)\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250523023425-0005/0 on hostPort 172.18.0.4:38149 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/23 02:34:25 INFO  BlockManagerMaster:60 Registered BlockManager BlockManagerId(driver, db74469cb85a, 42557, None)\n",
      "25/05/23 02:34:25 INFO  BlockManager:60 Initialized BlockManager: BlockManagerId(driver, db74469cb85a, 42557, None)\n",
      "25/05/23 02:34:25 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250523023425-0005/1 on worker-20250523021809-172.18.0.7-40709 (172.18.0.7:40709) with 4 core(s)\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250523023425-0005/1 on hostPort 172.18.0.7:40709 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/23 02:34:25 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250523023425-0005/2 on worker-20250523021809-172.18.0.5-42613 (172.18.0.5:42613) with 4 core(s)\n",
      "25/05/23 02:34:25 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250523023425-0005/2 on hostPort 172.18.0.5:42613 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/23 02:34:26 INFO  SingleEventLogFileWriter:60 Logging events to file:/opt/spark/spark-events/app-20250523023425-0005.inprogress\n",
      "25/05/23 02:34:26 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250523023425-0005/1 is now RUNNING\n",
      "25/05/23 02:34:26 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250523023425-0005/0 is now RUNNING\n",
      "25/05/23 02:34:26 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250523023425-0005/2 is now RUNNING\n",
      "25/05/23 02:34:26 INFO  StandaloneSchedulerBackend:60 SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Init DB\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c583868b-c9b4-4180-a3c5-855eea5e2b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/23 02:34:26 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('/opt/spark/spark-warehouse/') to the value of spark.sql.warehouse.dir.\n",
      "25/05/23 02:34:26 INFO  SharedState:60 Warehouse path is 's3a://delta-datawarehouse'.\n",
      "25/05/23 02:34:31 INFO  HiveUtils:60 Initializing HiveMetastoreConnection version 2.3.9 using file:/opt/spark/jars/jersey-container-servlet-2.40.jar:file:/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-aarch_64.jar:file:/opt/spark/jars/parquet-common-1.13.1.jar:file:/opt/spark/jars/kubernetes-model-events-6.7.2.jar:file:/opt/spark/jars/jersey-client-2.40.jar:file:/opt/spark/jars/hive-shims-common-2.3.9.jar:file:/opt/spark/jars/py4j-0.10.9.7.jar:file:/opt/spark/jars/jackson-mapper-asl-1.9.13.jar:file:/opt/spark/jars/jaxb-runtime-2.3.2.jar:file:/opt/spark/jars/jakarta.annotation-api-1.3.5.jar:file:/opt/spark/jars/spark-mesos_2.12-3.5.5.jar:file:/opt/spark/jars/json-1.8.jar:file:/opt/spark/jars/netty-handler-4.1.96.Final.jar:file:/opt/spark/jars/RoaringBitmap-0.9.45.jar:file:/opt/spark/jars/kubernetes-model-discovery-6.7.2.jar:file:/opt/spark/jars/bonecp-0.8.0.RELEASE.jar:file:/opt/spark/jars/cats-kernel_2.12-2.1.1.jar:file:/opt/spark/jars/netty-common-4.1.96.Final.jar:file:/opt/spark/jars/parquet-column-1.13.1.jar:file:/opt/spark/jars/javax.jdo-3.2.0-m3.jar:file:/opt/spark/jars/jodd-core-3.5.2.jar:file:/opt/spark/jars/commons-collections-3.2.2.jar:file:/opt/spark/jars/tink-1.9.0.jar:file:/opt/spark/jars/hive-cli-2.3.9.jar:file:/opt/spark/jars/guava-14.0.1.jar:file:/opt/spark/jars/blas-3.0.3.jar:file:/opt/spark/jars/snappy-java-1.1.10.5.jar:file:/opt/spark/jars/json4s-core_2.12-3.7.0-M11.jar:file:/opt/spark/jars/kubernetes-model-storageclass-6.7.2.jar:file:/opt/spark/jars/jackson-datatype-jsr310-2.15.2.jar:file:/opt/spark/jars/hive-serde-2.3.9.jar:file:/opt/spark/jars/spark-yarn_2.12-3.5.5.jar:file:/opt/spark/jars/jersey-common-2.40.jar:file:/opt/spark/jars/hk2-utils-2.6.1.jar:file:/opt/spark/jars/netty-codec-4.1.96.Final.jar:file:/opt/spark/jars/kubernetes-model-node-6.7.2.jar:file:/opt/spark/jars/commons-cli-1.5.0.jar:file:/opt/spark/jars/spark-launcher_2.12-3.5.5.jar:file:/opt/spark/jars/jersey-container-servlet-core-2.40.jar:file:/opt/spark/jars/parquet-hadoop-1.13.1.jar:file:/opt/spark/jars/netty-buffer-4.1.96.Final.jar:file:/opt/spark/jars/JLargeArrays-1.5.jar:file:/opt/spark/jars/okhttp-3.12.12.jar:file:/opt/spark/jars/aopalliance-repackaged-2.6.1.jar:file:/opt/spark/jars/hive-service-rpc-3.1.3.jar:file:/opt/spark/jars/antlr-runtime-3.5.2.jar:file:/opt/spark/jars/hive-exec-2.3.9-core.jar:file:/opt/spark/jars/spark-mllib-local_2.12-3.5.5.jar:file:/opt/spark/jars/metrics-jvm-4.2.19.jar:file:/opt/spark/jars/metrics-graphite-4.2.19.jar:file:/opt/spark/jars/hive-beeline-2.3.9.jar:file:/opt/spark/jars/jakarta.xml.bind-api-2.3.2.jar:file:/opt/spark/jars/xz-1.9.jar:file:/opt/spark/jars/breeze-macros_2.12-2.1.0.jar:file:/opt/spark/jars/compress-lzf-1.1.2.jar:file:/opt/spark/jars/kubernetes-client-6.7.2.jar:file:/opt/spark/jars/log4j-slf4j2-impl-2.20.0.jar:file:/opt/spark/jars/zookeeper-jute-3.6.3.jar:file:/opt/spark/jars/commons-dbcp-1.4.jar:file:/opt/spark/jars/spark-common-utils_2.12-3.5.5.jar:file:/opt/spark/jars/spark-sql-api_2.12-3.5.5.jar:file:/opt/spark/jars/leveldbjni-all-1.8.jar:file:/opt/spark/jars/log4j-core-2.20.0.jar:file:/opt/spark/jars/lapack-3.0.3.jar:file:/opt/spark/jars/javassist-3.29.2-GA.jar:file:/opt/spark/jars/commons-compiler-3.1.9.jar:file:/opt/spark/jars/ST4-4.0.4.jar:file:/opt/spark/jars/scala-compiler-2.12.18.jar:file:/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-x86_64.jar:file:/opt/spark/jars/netty-transport-native-epoll-4.1.96.Final-linux-aarch_64.jar:file:/opt/spark/jars/spire_2.12-0.17.0.jar:file:/opt/spark/jars/datanucleus-core-4.1.17.jar:file:/opt/spark/jars/lz4-java-1.8.0.jar:file:/opt/spark/jars/audience-annotations-0.5.0.jar:file:/opt/spark/jars/spark-network-common_2.12-3.5.5.jar:file:/opt/spark/jars/kubernetes-model-coordination-6.7.2.jar:file:/opt/spark/jars/istack-commons-runtime-3.0.8.jar:file:/opt/spark/jars/hadoop-yarn-server-web-proxy-3.3.4.jar:file:/opt/spark/jars/spark-hive_2.12-3.5.5.jar:file:/opt/spark/jars/parquet-jackson-1.13.1.jar:file:/opt/spark/jars/arrow-vector-12.0.1.jar:file:/opt/spark/jars/chill_2.12-0.10.0.jar:file:/opt/spark/jars/avro-1.11.4.jar:file:/opt/spark/jars/metrics-jmx-4.2.19.jar:file:/opt/spark/jars/scala-reflect-2.12.18.jar:file:/opt/spark/jars/hadoop-client-api-3.3.4.jar:file:/opt/spark/jars/hive-shims-2.3.9.jar:file:/opt/spark/jars/objenesis-3.3.jar:file:/opt/spark/jars/commons-logging-1.1.3.jar:file:/opt/spark/jars/arrow-memory-core-12.0.1.jar:file:/opt/spark/jars/spark-sketch_2.12-3.5.5.jar:file:/opt/spark/jars/commons-collections4-4.4.jar:file:/opt/spark/jars/kubernetes-model-gatewayapi-6.7.2.jar:file:/opt/spark/jars/orc-mapreduce-1.9.5-shaded-protobuf.jar:file:/opt/spark/jars/commons-pool-1.5.4.jar:file:/opt/spark/jars/netty-codec-socks-4.1.96.Final.jar:file:/opt/spark/jars/scala-library-2.12.18.jar:file:/opt/spark/jars/jakarta.ws.rs-api-2.1.6.jar:file:/opt/spark/jars/httpclient-4.5.14.jar:file:/opt/spark/jars/spark-mllib_2.12-3.5.5.jar:file:/opt/spark/jars/parquet-format-structures-1.13.1.jar:file:/opt/spark/jars/jackson-core-asl-1.9.13.jar:file:/opt/spark/jars/arpack-3.0.3.jar:file:/opt/spark/jars/kubernetes-model-apps-6.7.2.jar:file:/opt/spark/jars/avro-ipc-1.11.4.jar:file:/opt/spark/jars/kubernetes-model-scheduling-6.7.2.jar:file:/opt/spark/jars/arrow-format-12.0.1.jar:file:/opt/spark/jars/spark-unsafe_2.12-3.5.5.jar:file:/opt/spark/jars/scala-parser-combinators_2.12-2.3.0.jar:file:/opt/spark/jars/snakeyaml-2.0.jar:file:/opt/spark/jars/jackson-dataformat-yaml-2.15.2.jar:file:/opt/spark/jars/paranamer-2.8.jar:file:/opt/spark/jars/spark-tags_2.12-3.5.5.jar:file:/opt/spark/jars/jul-to-slf4j-2.0.7.jar:file:/opt/spark/jars/datasketches-java-3.3.0.jar:file:/opt/spark/jars/log4j-api-2.20.0.jar:file:/opt/spark/jars/xbean-asm9-shaded-4.23.jar:file:/opt/spark/jars/spire-macros_2.12-0.17.0.jar:file:/opt/spark/jars/osgi-resource-locator-1.0.3.jar:file:/opt/spark/jars/shims-0.9.45.jar:file:/opt/spark/jars/kubernetes-model-extensions-6.7.2.jar:file:/opt/spark/jars/javolution-5.5.1.jar:file:/opt/spark/jars/netty-transport-native-kqueue-4.1.96.Final-osx-x86_64.jar:file:/opt/spark/jars/kubernetes-model-batch-6.7.2.jar:file:/opt/spark/jars/netty-codec-http-4.1.96.Final.jar:file:/opt/spark/jars/jpam-1.1.jar:file:/opt/spark/jars/commons-io-2.16.1.jar:file:/opt/spark/jars/mesos-1.4.3-shaded-protobuf.jar:file:/opt/spark/jars/scala-collection-compat_2.12-2.7.0.jar:file:/opt/spark/jars/jdo-api-3.0.1.jar:file:/opt/spark/jars/log4j-1.2-api-2.20.0.jar:file:/opt/spark/jars/datasketches-memory-2.1.0.jar:file:/opt/spark/jars/snakeyaml-engine-2.6.jar:file:/opt/spark/jars/pickle-1.3.jar:file:/opt/spark/jars/hk2-api-2.6.1.jar:file:/opt/spark/jars/chill-java-0.10.0.jar:file:/opt/spark/jars/zstd-jni-1.5.5-4.jar:file:/opt/spark/jars/json4s-scalap_2.12-3.7.0-M11.jar:file:/opt/spark/jars/kubernetes-model-autoscaling-6.7.2.jar:file:/opt/spark/jars/spark-graphx_2.12-3.5.5.jar:file:/opt/spark/jars/spark-kubernetes_2.12-3.5.5.jar:file:/opt/spark/jars/JTransforms-3.1.jar:file:/opt/spark/jars/univocity-parsers-2.9.1.jar:file:/opt/spark/jars/spark-core_2.12-3.5.5.jar:file:/opt/spark/jars/commons-crypto-1.1.0.jar:file:/opt/spark/jars/commons-math3-3.6.1.jar:file:/opt/spark/jars/kubernetes-model-metrics-6.7.2.jar:file:/opt/spark/jars/curator-recipes-2.13.0.jar:file:/opt/spark/jars/kubernetes-model-common-6.7.2.jar:file:/opt/spark/jars/ivy-2.5.1.jar:file:/opt/spark/jars/kubernetes-model-networking-6.7.2.jar:file:/opt/spark/jars/arpack_combined_all-0.1.jar:file:/opt/spark/jars/hive-llap-common-2.3.9.jar:file:/opt/spark/jars/commons-lang-2.6.jar:file:/opt/spark/jars/zookeeper-3.6.3.jar:file:/opt/spark/jars/okio-1.17.6.jar:file:/opt/spark/jars/joda-time-2.12.5.jar:file:/opt/spark/jars/breeze_2.12-2.1.0.jar:file:/opt/spark/jars/hive-common-2.3.9.jar:file:/opt/spark/jars/spark-streaming_2.12-3.5.5.jar:file:/opt/spark/jars/transaction-api-1.1.jar:file:/opt/spark/jars/curator-framework-2.13.0.jar:file:/opt/spark/jars/hive-shims-0.23-2.3.9.jar:file:/opt/spark/jars/HikariCP-2.5.1.jar:file:/opt/spark/jars/kubernetes-model-certificates-6.7.2.jar:file:/opt/spark/jars/spire-platform_2.12-0.17.0.jar:file:/opt/spark/jars/httpcore-4.4.16.jar:file:/opt/spark/jars/activation-1.1.1.jar:file:/opt/spark/jars/spark-repl_2.12-3.5.5.jar:file:/opt/spark/jars/kryo-shaded-4.0.2.jar:file:/opt/spark/jars/orc-core-1.9.5-shaded-protobuf.jar:file:/opt/spark/jars/parquet-encoding-1.13.1.jar:file:/opt/spark/jars/netty-resolver-4.1.96.Final.jar:file:/opt/spark/jars/netty-all-4.1.96.Final.jar:file:/opt/spark/jars/netty-transport-classes-epoll-4.1.96.Final.jar:file:/opt/spark/jars/jakarta.servlet-api-4.0.3.jar:file:/opt/spark/jars/orc-shims-1.9.5.jar:file:/opt/spark/jars/commons-codec-1.16.1.jar:file:/opt/spark/jars/json4s-jackson_2.12-3.7.0-M11.jar:file:/opt/spark/jars/flatbuffers-java-1.12.0.jar:file:/opt/spark/jars/spark-sql_2.12-3.5.5.jar:file:/opt/spark/jars/netty-transport-native-unix-common-4.1.96.Final.jar:file:/opt/spark/jars/hive-storage-api-2.8.1.jar:file:/opt/spark/jars/hive-shims-scheduler-2.3.9.jar:file:/opt/spark/jars/jersey-server-2.40.jar:file:/opt/spark/jars/datanucleus-rdbms-4.1.19.jar:file:/opt/spark/jars/metrics-core-4.2.19.jar:file:/opt/spark/jars/jakarta.inject-2.6.1.jar:file:/opt/spark/jars/kubernetes-model-apiextensions-6.7.2.jar:file:/opt/spark/jars/aircompressor-0.27.jar:file:/opt/spark/jars/jackson-annotations-2.15.2.jar:file:/opt/spark/jars/hive-metastore-2.3.9.jar:file:/opt/spark/jars/metrics-json-4.2.19.jar:file:/opt/spark/jars/jsr305-3.0.0.jar:file:/opt/spark/jars/kubernetes-model-resource-6.7.2.jar:file:/opt/spark/jars/jline-2.14.6.jar:file:/opt/spark/jars/arrow-memory-netty-12.0.1.jar:file:/opt/spark/jars/datanucleus-api-jdo-4.2.4.jar:file:/opt/spark/jars/scala-xml_2.12-2.1.0.jar:file:/opt/spark/jars/kubernetes-httpclient-okhttp-6.7.2.jar:file:/opt/spark/jars/spark-kvstore_2.12-3.5.5.jar:file:/opt/spark/jars/minlog-1.3.0.jar:file:/opt/spark/jars/hadoop-shaded-guava-1.1.1.jar:file:/opt/spark/jars/jackson-module-scala_2.12-2.15.2.jar:file:/opt/spark/jars/kubernetes-model-rbac-6.7.2.jar:file:/opt/spark/jars/janino-3.1.9.jar:file:/opt/spark/jars/rocksdbjni-8.3.2.jar:file:/opt/spark/jars/oro-2.0.8.jar:file:/opt/spark/jars/libfb303-0.9.3.jar:file:/opt/spark/jars/spark-catalyst_2.12-3.5.5.jar:file:/opt/spark/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:file:/opt/spark/jars/commons-text-1.10.0.jar:file:/opt/spark/jars/jersey-hk2-2.40.jar:file:/opt/spark/jars/curator-client-2.13.0.jar:file:/opt/spark/jars/jcl-over-slf4j-2.0.7.jar:file:/opt/spark/jars/json4s-ast_2.12-3.7.0-M11.jar:file:/opt/spark/jars/netty-codec-http2-4.1.96.Final.jar:file:/opt/spark/jars/kubernetes-model-admissionregistration-6.7.2.jar:file:/opt/spark/jars/netty-handler-proxy-4.1.96.Final.jar:file:/opt/spark/jars/zjsonpatch-0.3.0.jar:file:/opt/spark/jars/logging-interceptor-3.12.12.jar:file:/opt/spark/jars/commons-compress-1.23.0.jar:file:/opt/spark/jars/annotations-17.0.0.jar:file:/opt/spark/jars/kubernetes-model-flowcontrol-6.7.2.jar:file:/opt/spark/jars/libthrift-0.12.0.jar:file:/opt/spark/jars/jta-1.1.jar:file:/opt/spark/jars/hive-jdbc-2.3.9.jar:file:/opt/spark/jars/jackson-databind-2.15.2.jar:file:/opt/spark/jars/slf4j-api-2.0.7.jar:file:/opt/spark/jars/super-csv-2.2.0.jar:file:/opt/spark/jars/stax-api-1.0.1.jar:file:/opt/spark/jars/kubernetes-model-policy-6.7.2.jar:file:/opt/spark/jars/hk2-locator-2.6.1.jar:file:/opt/spark/jars/threeten-extra-1.7.1.jar:file:/opt/spark/jars/spark-network-shuffle_2.12-3.5.5.jar:file:/opt/spark/jars/kubernetes-client-api-6.7.2.jar:file:/opt/spark/jars/netty-transport-4.1.96.Final.jar:file:/opt/spark/jars/jackson-core-2.15.2.jar:file:/opt/spark/jars/spark-hive-thriftserver_2.12-3.5.5.jar:file:/opt/spark/jars/spire-util_2.12-0.17.0.jar:file:/opt/spark/jars/avro-mapred-1.11.4.jar:file:/opt/spark/jars/opencsv-2.3.jar:file:/opt/spark/jars/antlr4-runtime-4.9.3.jar:file:/opt/spark/jars/kubernetes-model-core-6.7.2.jar:file:/opt/spark/jars/netty-transport-classes-kqueue-4.1.96.Final.jar:file:/opt/spark/jars/jakarta.validation-api-2.0.2.jar:file:/opt/spark/jars/hadoop-client-runtime-3.3.4.jar:file:/opt/spark/jars/stream-2.9.6.jar:file:/opt/spark/jars/derby-10.14.2.0.jar:file:/opt/spark/jars/gson-2.2.4.jar:file:/opt/spark/jars/algebra_2.12-2.0.1.jar:file:/opt/spark/jars/commons-lang3-3.12.0.jar:file:/opt/spark/jars/openlineage-spark_2.12-1.32.0.jar:file:/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.5.jar:file:/opt/spark/jars/hadoop-aws-3.3.4.jar:file:/opt/spark/jars/aws-java-sdk-bundle-1.12.782.jar:file:/opt/spark/jars/postgresql-42.7.5.jar:file:/opt/spark/jars/delta-spark_2.12-3.3.1.jar:file:/opt/spark/jars/delta-storage-3.3.1.jar:file:/opt/spark/jars/delta-core_2.12-2.4.0.jar\n",
      "25/05/23 02:34:32 INFO  HiveConf:187 Found configuration file file:/opt/spark/conf/hive-site.xml\n",
      "25/05/23 02:34:32 INFO  HiveClientImpl:60 Warehouse location for Hive client (version 2.3.9) is s3a://delta-datawarehouse\n",
      "25/05/23 02:34:33 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/23 02:34:33 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/23 02:34:33 INFO  HiveMetaStore:614 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "25/05/23 02:34:33 INFO  ObjectStore:403 ObjectStore, initialize called\n",
      "25/05/23 02:34:34 INFO  Persistence:77 Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "25/05/23 02:34:34 INFO  Persistence:77 Property datanucleus.cache.level2 unknown - will be ignored\n",
      "25/05/23 02:34:35 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:35544) with ID 0,  ResourceProfileId 0\n",
      "25/05/23 02:34:35 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.4:33665 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.4, 33665, None)\n",
      "25/05/23 02:34:35 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.5:35934) with ID 2,  ResourceProfileId 0\n",
      "25/05/23 02:34:35 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.7:47980) with ID 1,  ResourceProfileId 0\n",
      "25/05/23 02:34:35 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.5:41941 with 1048.8 MiB RAM, BlockManagerId(2, 172.18.0.5, 41941, None)\n",
      "25/05/23 02:34:36 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.7:46581 with 1048.8 MiB RAM, BlockManagerId(1, 172.18.0.7, 46581, None)\n",
      "25/05/23 02:34:44 INFO  ObjectStore:526 Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "25/05/23 02:34:46 INFO  MetaStoreDirectSql:146 Using direct SQL, underlying DB is DERBY\n",
      "25/05/23 02:34:46 INFO  ObjectStore:317 Initialized ObjectStore\n",
      "25/05/23 02:34:46 WARN  ObjectStore:7812 Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/05/23 02:34:46 WARN  ObjectStore:7900 setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.3\n",
      "25/05/23 02:34:46 WARN  ObjectStore:723 Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create dw schema in catalog\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreate database if not exists edw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mcreate database if not exists edw_stg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mcreate database if not exists edw_ld\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "# Create dw schema in catalog\n",
    "spark.sql(\"create database if not exists edw\")\n",
    "spark.sql(\"create database if not exists edw_stg\")\n",
    "spark.sql(\"create database if not exists edw_ld\")\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02bf715-0858-40cd-9f02-f18976b213c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Store Dim table\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_store\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_store (\n",
    "    row_wid string,\n",
    "    store_id string,\n",
    "    store_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Store dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b02d7-525b-4919-9b89-b43c51b700c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Plan Type Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_plan_type\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_plan_type (\n",
    "    plan_type_code string,\n",
    "    plan_name string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Plan Type dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee98612-192f-4e55-87cc-e87c7b4b7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Date Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_date\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_date (\n",
    "    row_wid string,\n",
    "    date date,\n",
    "    day int,\n",
    "    month int,\n",
    "    year int,\n",
    "    day_of_week string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Date dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ca64-287f-48b9-9c50-67b43524621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Product Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_product\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_product (\n",
    "    row_wid string,\n",
    "    product_id string,\n",
    "    product_name string,\n",
    "    brand string,\n",
    "    type string,\n",
    "    flavor string,\n",
    "    size string,\n",
    "    price double,\n",
    "    expiration_dt date,\n",
    "    image_url string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_dt timestamp,\n",
    "    active_flg int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Product dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adc179-6965-4c55-9d85-e86397b8d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Customer Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_customer\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_customer (\n",
    "    row_wid string,\n",
    "    customer_id string,\n",
    "    first_name string,\n",
    "    last_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    email string,\n",
    "    date_of_birth date,\n",
    "    plan_type string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_dt timestamp,\n",
    "    active_flg int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Customer dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8a620-ab75-457f-a893-f758c3eda687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sales Fact\n",
    "spark.sql(\"\"\"drop table if exists edw.fact_sales\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.fact_sales (\n",
    "    date_wid string,\n",
    "    product_wid string,\n",
    "    store_wid string,\n",
    "    customer_wid string,\n",
    "    order_id string,\n",
    "    invoice_num string,\n",
    "    qty int,\n",
    "    tax double,\n",
    "    discount double,\n",
    "    line_total double,\n",
    "    integration_key string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Sales Fact created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97508f9-0144-422d-bf8b-5acfdb556f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Audit table\n",
    "spark.sql(\"\"\"drop table if exists edw.job_control\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.job_control (\n",
    "    schema_name string,\n",
    "    table_name string,\n",
    "    max_timestamp timestamp,\n",
    "    rundate string,\n",
    "    insert_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: JOB Control table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd652a-5d06-40d9-bd20-22051330fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all tables in Data Warehouse/Lakehouse\n",
    "\n",
    "spark.sql(\"show tables in edw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f2976-7182-44d7-a309-e375dc7bf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d935781-3fdb-44ab-b936-7b26674815a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
