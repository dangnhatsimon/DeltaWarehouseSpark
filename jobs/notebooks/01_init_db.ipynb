{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a409d3d-41a6-47ec-9474-b392da225d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, trim, lower, expr\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "import logging\n",
    "from os.path import abspath\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional, Union, List, Tuple, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1022aabc-ae7a-4d69-86de-4fc9f9bc3c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.4\n"
     ]
    }
   ],
   "source": [
    "print(sc._jvm.org.apache.hadoop.util.VersionInfo.getVersion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf1d053-f736-4c10-a0bc-f3e579a5568c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:derby:;databaseName=/opt/spark/warehouse/metastore_db;create=true\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.hadoopConfiguration().get(\"javax.jdo.option.ConnectionURL\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "843b423d-3ba9-48fa-b11b-b9f74a986282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark/warehouse/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:44:22 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:52364) with ID 1,  ResourceProfileId 0\n",
      "25/05/20 13:44:22 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:36028) with ID 2,  ResourceProfileId 0\n",
      "25/05/20 13:44:22 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:59772) with ID 0,  ResourceProfileId 0\n",
      "25/05/20 13:44:22 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.3:45939 with 1048.8 MiB RAM, BlockManagerId(1, 172.18.0.3, 45939, None)\n",
      "25/05/20 13:44:22 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.6:37821 with 1048.8 MiB RAM, BlockManagerId(2, 172.18.0.6, 37821, None)\n",
      "25/05/20 13:44:22 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.4:38425 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.4, 38425, None)\n"
     ]
    }
   ],
   "source": [
    "print(spark._jsc.hadoopConfiguration().get(\"hive.metastore.warehouse.dir\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d9b200-04b2-4bf9-89e3-e648f52c210b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:44:38 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('/opt/spark/warehouse/') to the value of spark.sql.warehouse.dir.\n",
      "25/05/20 13:44:38 WARN  MetricsConfig:136 Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/05/20 13:44:38 INFO  MetricsSystemImpl:378 Scheduled Metric snapshot period at 10 second(s).\n",
      "25/05/20 13:44:38 INFO  MetricsSystemImpl:191 s3a-file-system metrics system started\n",
      "25/05/20 13:44:38 WARN  VersionInfoUtils:85 The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1619)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1258)\n",
      "at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:898)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.spark.sql.internal.SharedState$.qualifyWarehousePath(SharedState.scala:288)\n",
      "at org.apache.spark.sql.internal.SharedState.liftedTree1$1(SharedState.scala:80)\n",
      "at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:79)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:143)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:143)\n",
      "at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:142)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n",
      "at scala.Option.getOrElse(Option.scala:189)\n",
      "at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n",
      "at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n",
      "at org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n",
      "at org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/20 13:44:39 INFO  SharedState:60 Warehouse path is 's3a://delta-datawarehouse'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hive\n"
     ]
    }
   ],
   "source": [
    "print(spark.conf.get(\"spark.sql.catalogImplementation\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f21b504-be39-40df-9a4b-5bddb8355658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:44:40 INFO  SparkContext:60 SparkContext is stopping with exitCode 0.\n",
      "25/05/20 13:44:40 INFO  SparkUI:60 Stopped Spark web UI at http://d0f9a55c06b4:4040\n",
      "25/05/20 13:44:40 INFO  StandaloneSchedulerBackend:60 Shutting down all executors\n",
      "25/05/20 13:44:40 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Asking each executor to shut down\n",
      "25/05/20 13:44:40 INFO  MapOutputTrackerMasterEndpoint:60 MapOutputTrackerMasterEndpoint stopped!\n",
      "25/05/20 13:44:40 INFO  MemoryStore:60 MemoryStore cleared\n",
      "25/05/20 13:44:40 INFO  BlockManager:60 BlockManager stopped\n",
      "25/05/20 13:44:40 INFO  BlockManagerMaster:60 BlockManagerMaster stopped\n",
      "25/05/20 13:44:40 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:60 OutputCommitCoordinator stopped!\n",
      "25/05/20 13:44:40 INFO  SparkContext:60 Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a6825187ec98f97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:44:41 INFO  SparkContext:60 Running Spark version 3.5.5\n",
      "25/05/20 13:44:41 INFO  SparkContext:60 OS info Linux, 5.15.167.4-microsoft-standard-WSL2, amd64\n",
      "25/05/20 13:44:41 INFO  SparkContext:60 Java version 17.0.14\n",
      "Setting Spark log level to \"INFO\".\n",
      "25/05/20 13:44:41 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/20 13:44:41 INFO  ResourceUtils:60 No custom resources configured for spark.driver.\n",
      "25/05/20 13:44:41 INFO  ResourceUtils:60 ==============================================================\n",
      "25/05/20 13:44:41 INFO  SparkContext:60 Submitted application: Init DB\n",
      "25/05/20 13:44:41 INFO  ResourceProfile:60 Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 4, script: , vendor: , memory -> name: memory, amount: 2048, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "25/05/20 13:44:41 INFO  ResourceProfile:60 Limiting resource is cpus at 4 tasks per executor\n",
      "25/05/20 13:44:41 INFO  ResourceProfileManager:60 Added ResourceProfile id: 0\n",
      "25/05/20 13:44:41 INFO  SecurityManager:60 Changing view acls to: root\n",
      "25/05/20 13:44:41 INFO  SecurityManager:60 Changing modify acls to: root\n",
      "25/05/20 13:44:41 INFO  SecurityManager:60 Changing view acls groups to: \n",
      "25/05/20 13:44:41 INFO  SecurityManager:60 Changing modify acls groups to: \n",
      "25/05/20 13:44:41 INFO  SecurityManager:60 SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
      "25/05/20 13:44:41 INFO  Utils:60 Successfully started service 'sparkDriver' on port 40297.\n",
      "25/05/20 13:44:41 INFO  SparkEnv:60 Registering MapOutputTracker\n",
      "25/05/20 13:44:41 INFO  SparkEnv:60 Registering BlockManagerMaster\n",
      "25/05/20 13:44:41 INFO  BlockManagerMasterEndpoint:60 Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "25/05/20 13:44:41 INFO  BlockManagerMasterEndpoint:60 BlockManagerMasterEndpoint up\n",
      "25/05/20 13:44:41 INFO  SparkEnv:60 Registering BlockManagerMasterHeartbeat\n",
      "25/05/20 13:44:41 INFO  DiskBlockManager:60 Created local directory at /tmp/blockmgr-86e0db5a-984d-469b-a584-f44d7550b6c3\n",
      "25/05/20 13:44:41 INFO  MemoryStore:60 MemoryStore started with capacity 434.4 MiB\n",
      "25/05/20 13:44:41 INFO  SparkEnv:60 Registering OutputCommitCoordinator\n",
      "25/05/20 13:44:41 INFO  JettyUtils:60 Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "25/05/20 13:44:41 INFO  Utils:60 Successfully started service 'SparkUI' on port 4040.\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Connecting to master spark://delta-warehouse-spark-master:7077...\n",
      "25/05/20 13:44:41 INFO  TransportClientFactory:316 Successfully created connection to delta-warehouse-spark-master/172.18.0.2:7077 after 0 ms (0 ms spent in bootstraps)\n",
      "25/05/20 13:44:41 INFO  StandaloneSchedulerBackend:60 Connected to Spark cluster with app ID app-20250520134441-0011\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250520134441-0011/0 on worker-20250520110746-172.18.0.4-38445 (172.18.0.4:38445) with 4 core(s)\n",
      "25/05/20 13:44:41 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250520134441-0011/0 on hostPort 172.18.0.4:38445 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250520134441-0011/1 on worker-20250520110746-172.18.0.3-44063 (172.18.0.3:44063) with 4 core(s)\n",
      "25/05/20 13:44:41 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250520134441-0011/1 on hostPort 172.18.0.3:44063 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor added: app-20250520134441-0011/2 on worker-20250520110746-172.18.0.6-36819 (172.18.0.6:36819) with 4 core(s)\n",
      "25/05/20 13:44:41 INFO  StandaloneSchedulerBackend:60 Granted executor ID app-20250520134441-0011/2 on hostPort 172.18.0.6:36819 with 4 core(s), 2.0 GiB RAM\n",
      "25/05/20 13:44:41 INFO  Utils:60 Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46827.\n",
      "25/05/20 13:44:41 INFO  NettyBlockTransferService:84 Server created on d0f9a55c06b4:46827\n",
      "25/05/20 13:44:41 INFO  BlockManager:60 Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "25/05/20 13:44:41 INFO  BlockManagerMaster:60 Registering BlockManager BlockManagerId(driver, d0f9a55c06b4, 46827, None)\n",
      "25/05/20 13:44:41 INFO  BlockManagerMasterEndpoint:60 Registering block manager d0f9a55c06b4:46827 with 434.4 MiB RAM, BlockManagerId(driver, d0f9a55c06b4, 46827, None)\n",
      "25/05/20 13:44:41 INFO  BlockManagerMaster:60 Registered BlockManager BlockManagerId(driver, d0f9a55c06b4, 46827, None)\n",
      "25/05/20 13:44:41 INFO  BlockManager:60 Initialized BlockManager: BlockManagerId(driver, d0f9a55c06b4, 46827, None)\n",
      "25/05/20 13:44:41 INFO  SingleEventLogFileWriter:60 Logging events to file:/opt/spark/spark-events/app-20250520134441-0011.inprogress\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250520134441-0011/0 is now RUNNING\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250520134441-0011/1 is now RUNNING\n",
      "25/05/20 13:44:41 INFO  StandaloneAppClient$ClientEndpoint:60 Executor updated: app-20250520134441-0011/2 is now RUNNING\n",
      "25/05/20 13:44:41 INFO  StandaloneSchedulerBackend:60 SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Init DB\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c583868b-c9b4-4180-a3c5-855eea5e2b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 13:44:42 INFO  SharedState:60 Setting hive.metastore.warehouse.dir ('/opt/spark/warehouse/') to the value of spark.sql.warehouse.dir.\n",
      "25/05/20 13:44:42 INFO  SharedState:60 Warehouse path is 's3a://delta-datawarehouse'.\n",
      "25/05/20 13:44:44 INFO  HiveUtils:60 Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.\n",
      "25/05/20 13:44:43 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/20 13:44:43 WARN  HiveConf:4122 HiveConf of name hive.metastore.schema.version does not exist\n",
      "25/05/20 13:44:43 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/20 13:44:43 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.3:40860) with ID 1,  ResourceProfileId 0\n",
      "25/05/20 13:44:43 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.6:44940) with ID 2,  ResourceProfileId 0\n",
      "25/05/20 13:44:43 INFO  HiveClientImpl:60 Warehouse location for Hive client (version 2.3.9) is s3a://delta-datawarehouse\n",
      "25/05/20 13:44:43 INFO  StandaloneSchedulerBackend$StandaloneDriverEndpoint:60 Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.4:33264) with ID 0,  ResourceProfileId 0\n",
      "25/05/20 13:44:43 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.3:45531 with 1048.8 MiB RAM, BlockManagerId(1, 172.18.0.3, 45531, None)\n",
      "25/05/20 13:44:43 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.6:34747 with 1048.8 MiB RAM, BlockManagerId(2, 172.18.0.6, 34747, None)\n",
      "25/05/20 13:44:43 INFO  BlockManagerMasterEndpoint:60 Registering block manager 172.18.0.4:39797 with 1048.8 MiB RAM, BlockManagerId(0, 172.18.0.4, 39797, None)\n",
      "25/05/20 13:44:44 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/20 13:44:44 WARN  HiveConf:4122 HiveConf of name hive.metastore.schema.version does not exist\n",
      "25/05/20 13:44:42 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/20 13:44:42 INFO  HiveMetaStore:614 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore\n",
      "25/05/20 13:44:42 INFO  ObjectStore:403 ObjectStore, initialize called\n",
      "25/05/20 13:44:43 INFO  Persistence:77 Property hive.metastore.integral.jdo.pushdown unknown - will be ignored\n",
      "25/05/20 13:44:43 INFO  Persistence:77 Property datanucleus.cache.level2 unknown - will be ignored\n",
      "25/05/20 13:44:46 WARN  HiveConf:4122 HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/05/20 13:44:46 WARN  HiveConf:4122 HiveConf of name hive.metastore.schema.version does not exist\n",
      "25/05/20 13:44:46 WARN  HiveConf:4122 HiveConf of name hive.stats.retries.wait does not exist\n",
      "25/05/20 13:44:46 INFO  ObjectStore:526 Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\"Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\"\n",
      "25/05/20 13:44:47 INFO  MetaStoreDirectSql:146 Using direct SQL, underlying DB is DERBY\n",
      "25/05/20 13:44:47 INFO  ObjectStore:317 Initialized ObjectStore\n",
      "25/05/20 13:44:47 WARN  ObjectStore:7812 Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0\n",
      "25/05/20 13:44:47 WARN  ObjectStore:7900 setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@172.18.0.2\n",
      "25/05/20 13:44:47 WARN  ObjectStore:723 Failed to get database default, returning NoSuchObjectException\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create dw schema in catalog\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcreate database if not exists edw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mcreate database if not exists edw_stg\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m spark.sql(\u001b[33m\"\u001b[39m\u001b[33mcreate database if not exists edw_ld\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "# Create dw schema in catalog\n",
    "spark.sql(\"create database if not exists edw\")\n",
    "spark.sql(\"create database if not exists edw_stg\")\n",
    "spark.sql(\"create database if not exists edw_ld\")\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a02bf715-0858-40cd-9f02-f18976b213c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/20 11:25:55 INFO  HiveMetaStore:781 0: get_database: global_temp\n",
      "25/05/20 11:25:55 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: global_temp\t\n",
      "25/05/20 11:25:55 WARN  ObjectStore:723 Failed to get database global_temp, returning NoSuchObjectException\n",
      "25/05/20 11:25:55 INFO  HiveMetaStore:781 0: get_database: edw\n",
      "25/05/20 11:25:55 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: edw\t\n",
      "25/05/20 11:25:55 INFO  HiveMetaStore:781 0: get_table : db=edw tbl=dim_store\n",
      "25/05/20 11:25:55 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=edw tbl=dim_store\t\n",
      "25/05/20 11:25:55 INFO  HiveMetaStore:781 0: get_database: edw\n",
      "25/05/20 11:25:55 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_database: edw\t\n",
      "25/05/20 11:25:55 INFO  HiveMetaStore:781 0: get_table : db=edw tbl=dim_store\n",
      "25/05/20 11:25:55 INFO  audit:309 ugi=root\tip=unknown-ip-addr\tcmd=get_table : db=edw tbl=dim_store\t\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o102.sql.\n: java.lang.NoClassDefFoundError: io/delta/storage/commit/actions/AbstractProtocol\n\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.delta.actions.Protocol$.apply(actions.scala:205)\n\tat org.apache.spark.sql.delta.actions.Action$.<init>(actions.scala:61)\n\tat org.apache.spark.sql.delta.actions.Action$.<clinit>(actions.scala)\n\tat org.apache.spark.sql.delta.DeltaConfigsBase.$init$(DeltaConfig.scala:334)\n\tat org.apache.spark.sql.delta.DeltaConfigs$.<init>(DeltaConfig.scala:858)\n\tat org.apache.spark.sql.delta.DeltaConfigs$.<clinit>(DeltaConfig.scala)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:129)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:103)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:370)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:350)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:341)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: io.delta.storage.commit.actions.AbstractProtocol\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 70 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create Store Dim table\u001b[39;00m\n\u001b[32m      2\u001b[39m spark.sql(\u001b[33m\"\"\"\u001b[39m\u001b[33mdrop table if exists edw.dim_store\u001b[39m\u001b[33m\"\"\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[33;43mcreate table edw.dim_store (\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[33;43m    row_wid string,\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[33;43m    store_id string,\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[33;43m    store_name string,\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[33;43m    address string,\u001b[39;49m\n\u001b[32m     10\u001b[39m \u001b[33;43m    city string,\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[33;43m    state string,\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[33;43m    zip_code string,\u001b[39;49m\n\u001b[32m     13\u001b[39m \u001b[33;43m    phone_number string,\u001b[39;49m\n\u001b[32m     14\u001b[39m \u001b[33;43m    rundate string,\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[33;43m    insert_dt timestamp,\u001b[39;49m\n\u001b[32m     16\u001b[39m \u001b[33;43m    update_dt timestamp\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[33;43m)\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[33;43mUSING delta\u001b[39;49m\n\u001b[32m     19\u001b[39m \u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSPARK-APP: Store dimension created\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[39m, in \u001b[36mSparkSession.sql\u001b[39m\u001b[34m(self, sqlQuery, args, **kwargs)\u001b[39m\n\u001b[32m   1627\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1628\u001b[39m         litArgs = \u001b[38;5;28mself\u001b[39m._jvm.PythonUtils.toArray(\n\u001b[32m   1629\u001b[39m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[32m   1630\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1631\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1632\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1633\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    324\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o102.sql.\n: java.lang.NoClassDefFoundError: io/delta/storage/commit/actions/AbstractProtocol\n\tat java.base/java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1017)\n\tat java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\tat org.apache.spark.sql.delta.actions.Protocol$.apply(actions.scala:205)\n\tat org.apache.spark.sql.delta.actions.Action$.<init>(actions.scala:61)\n\tat org.apache.spark.sql.delta.actions.Action$.<clinit>(actions.scala)\n\tat org.apache.spark.sql.delta.DeltaConfigsBase.$init$(DeltaConfig.scala:334)\n\tat org.apache.spark.sql.delta.DeltaConfigs$.<init>(DeltaConfig.scala:858)\n\tat org.apache.spark.sql.delta.DeltaConfigs$.<clinit>(DeltaConfig.scala)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:129)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.org$apache$spark$sql$delta$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:103)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.$anonfun$createTable$1(DeltaCatalog.scala:370)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:171)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:67)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:350)\n\tat org.apache.spark.sql.delta.catalog.DeltaCatalog.createTable(DeltaCatalog.scala:341)\n\tat org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.lang.ClassNotFoundException: io.delta.storage.commit.actions.AbstractProtocol\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n\t... 70 more\n"
     ]
    }
   ],
   "source": [
    "# Create Store Dim table\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_store\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_store (\n",
    "    row_wid string,\n",
    "    store_id string,\n",
    "    store_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Store dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b02d7-525b-4919-9b89-b43c51b700c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Plan Type Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_plan_type\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_plan_type (\n",
    "    plan_type_code string,\n",
    "    plan_name string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Plan Type dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee98612-192f-4e55-87cc-e87c7b4b7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Date Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_date\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_date (\n",
    "    row_wid string,\n",
    "    date date,\n",
    "    day int,\n",
    "    month int,\n",
    "    year int,\n",
    "    day_of_week string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Date dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a62ca64-287f-48b9-9c50-67b43524621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Product Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_product\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_product (\n",
    "    row_wid string,\n",
    "    product_id string,\n",
    "    product_name string,\n",
    "    brand string,\n",
    "    type string,\n",
    "    flavor string,\n",
    "    size string,\n",
    "    price double,\n",
    "    expiration_dt date,\n",
    "    image_url string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_dt timestamp,\n",
    "    active_flg int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Product dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1adc179-6965-4c55-9d85-e86397b8d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Customer Dimension\n",
    "spark.sql(\"\"\"drop table if exists edw.dim_customer\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.dim_customer (\n",
    "    row_wid string,\n",
    "    customer_id string,\n",
    "    first_name string,\n",
    "    last_name string,\n",
    "    address string,\n",
    "    city string,\n",
    "    state string,\n",
    "    zip_code string,\n",
    "    phone_number string,\n",
    "    email string,\n",
    "    date_of_birth date,\n",
    "    plan_type string,\n",
    "    effective_start_dt timestamp,\n",
    "    effective_end_dt timestamp,\n",
    "    active_flg int,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Customer dimension created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c8a620-ab75-457f-a893-f758c3eda687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sales Fact\n",
    "spark.sql(\"\"\"drop table if exists edw.fact_sales\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.fact_sales (\n",
    "    date_wid string,\n",
    "    product_wid string,\n",
    "    store_wid string,\n",
    "    customer_wid string,\n",
    "    order_id string,\n",
    "    invoice_num string,\n",
    "    qty int,\n",
    "    tax double,\n",
    "    discount double,\n",
    "    line_total double,\n",
    "    integration_key string,\n",
    "    rundate string,\n",
    "    insert_dt timestamp,\n",
    "    update_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: Sales Fact created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97508f9-0144-422d-bf8b-5acfdb556f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Audit table\n",
    "spark.sql(\"\"\"drop table if exists edw.job_control\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "create table edw.job_control (\n",
    "    schema_name string,\n",
    "    table_name string,\n",
    "    max_timestamp timestamp,\n",
    "    rundate string,\n",
    "    insert_dt timestamp\n",
    ")\n",
    "USING delta\n",
    ";\n",
    "\"\"\")\n",
    "\n",
    "print(\"SPARK-APP: JOB Control table created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd652a-5d06-40d9-bd20-22051330fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all tables in Data Warehouse/Lakehouse\n",
    "\n",
    "spark.sql(\"show tables in edw\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5f2976-7182-44d7-a309-e375dc7bf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d935781-3fdb-44ab-b936-7b26674815a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
